{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oh, no! We've had a data crash.\n",
    "As ChiPy leadership was preparing for [PyCon](https://us.pycon.org/2019/) at the end of this month, they found that the dataset on our infamous *ChiPy chipmunks* has disapeared. While they transition from Oracle to Postgres, the leadership team has enlisted your help as data scientists to analyze some salvaged chipmunk data. The PyCon organizers had a few questions about coding in Chicago, ChiPy, and chipmunks that need answers. We will get to those questions shortly, but first let's get to the data. If you haven't set up your environment, make sure to checkout the project night page on the [website](http://chicagopython.github.io/chipy-chipmunks).\n",
    "\n",
    "\n",
    "## Reading in the Data\n",
    "The salvaged chipmunk dataset is `chipmunk.csv`. The wonderful [pandas](https://pandas.pydata.org/) library, built on [numpy](http://www.numpy.org/), will let the team read in the data. \n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "Now is a good time to check in with the team. Is anyone familiar with `pandas` and `numpy`? Discuss with your team what these libraries are, what they allow data scientists to do, and then decide on what `pandas` function will read in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "We need to be familiar with our data before we can answer questions about ChiPy and our chipmunks. Let's start with some questions we would ask of *any* dataset:\n",
    "\n",
    "- How many rows are in this dataset? What does each row represent?\n",
    "- What does the data look like? Check the first 5 rows\n",
    "- Is there missing data? If so, how much is missing?\n",
    "- What columns are categorical?\n",
    "- What are the unique number of observations for each column?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## See first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check for categorical data and unique number of values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was there missing data?\n",
    "\n",
    "We will keep exploring the data and start answering questions soon, but first let's address missing data (if there is any). What columns have missing data? What kind of data is missing?\n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "This a great point for discussion. If there is missing data - why might it be missing? Discuss some possible reasons with your team and decide on a reason that makes sense.\n",
    "\n",
    "[Imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) is the process of replacing missing data with some estimated value. The process can be as complicated (or simple) as you would like it to be! Given the possible reason for our missing data, what is an acceptable imputation?\n",
    "\n",
    "Impute any missing data in your dataset and note what assumptions you made as a team. If you are not sure how to replace data in `pandas`, feel free to use google like a proper data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace any missing data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check your data for missing values to see if it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stakeholder Questions\n",
    "## Question #1\n",
    "\n",
    "The great folks at PyCon want to know all about ChiPy and our chipmunks. They have heard that **ChiPy is an inclusive and open community**. Can we support that claim with our data? Given that the `ChiPy` column takes a value of `1` for a ChiPy chipmunk and a value of `0` for chipmunks not in ChiPy, start to explore this question.\n",
    "\n",
    "Some ideas to get you started:\n",
    "- Are chipmunks of different species represented in ChiPy?\n",
    "- Are chipmunks of different sizes represented in ChiPy?\n",
    "- Are chipmunks of different careers represented in ChiPy?\n",
    "- Are spotted and not spotted chipmunks represented in ChiPy?\n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "There are no right or wrong answers here, only well supported or poorly supported ones! Discuss as a group the aspects of the data you have looked at and if it constitutes enough evidence to justify an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exploration of species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exploration of size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exploration of careers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exploration of spotted vs non-spotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "The word on the street at PyCon is that chipmunks that live in Chicago enjoy coding more than those that don't. Is this not true? Given that the `chicago` column takes a value of `1` for chipmunks that live in Chicago and a value of `0` for chipmunks that do not, explore this question.\n",
    "\n",
    "- Visualize the distributions of `coding_enjoyment` for chipmunks that do and do not live in Chicago.\n",
    "- Come up with a way to test our question.\n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "Coming up with a proper way to test stakeholder questions can be an artform as well as a science. We have imported a few statistical tests below that may (or may not) be appropriate for our question. First consider a way to frame our question as something to *disprove* (those familiar with jargon, let's construct a [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis)) - then conduct a test that may disprove it. Reading the documentation for the imported tests below may prove to be very helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, levene, chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Beautiful plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2, Continued\n",
    "\n",
    "We have now compared two groups of chipmunks - those that live in Chicago and those that do not - and have either rejected or failed to reject a null hypothesis. What values did the statistical test return and what do they mean? Can we be confident in our results? How confident?\n",
    "\n",
    "Regardless of our test results, what are the limitations of the test? One limitation is that we have information in our data that is *related* to being in Chicago and might also have an effect on enjoyment of coding. [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis) will allow us to examine the relationship between living in Chicago and enjoyment of coding while controlling for membership in ChiPy. Use the `statsmodels` package to regress `chicago` and `ChiPy` on `coding_enjoyment`. See [this example](https://www.statsmodels.org/stable/index.html) for assistance.\n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "This regression model still has limitations, and there could be an entire project night on this task alone. What steps would need to be taken if we controlled for more characteristics of our data?\n",
    "\n",
    "This is also a good time to discuss what kind of information we are looking for in our regression model. What are coefficients and what do they mean? What is a p-value? Is it similar to a p-value from the statistical tests above?\n",
    "\n",
    "Lastly, modeling is fun, but don't forget the original question! Do chipmunks that live in Chicago enjoy coding more than those that don't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression model and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3\n",
    "ChiPy leadership wants to send 20 lucky chipmunks to cheer the lovely folks at PyCon. They have asked us to make a predictive model that can be used on their chipmunk data when it is recovered. To do this we will:\n",
    "- Make a train/test split to evaluate our model\n",
    "- Scale our data\n",
    "- Fit several models\n",
    "- Decide on an evaluation metric\n",
    "- Select this best model\n",
    "\n",
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "The cell below transforms our data so that every feature (jargon for column) is numeric. Discuss with your team why this is could be an important step. Engineering features could also be an entire project night!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wide_data = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wide_data.drop('ChiPy', axis=1), \n",
    "                                                    wide_data.ChiPy, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #f92828;text-decoration: underline;\">ChiPy Check-in</h3>\n",
    "\n",
    "Having the proper evaluation metric is the most important process in predictive modeling. Below we have imported accuracy, precision, and recall. What are each of these metrics and when should they be used? Given that we want to give 20 PyCon tickets to only ChiPy chipmunks, which metric is most appropriate here? Black box evaluation methods like `classification_report` will not be helpful here given the constraint of only having 20 tickets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Evaluate models, optimizing your predictions for 20 chipmunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "nikola": {
   "category": "",
   "date": "2019-04-17 22:39:06 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "chipmunks-data-science",
   "tags": "",
   "title": "Chipmunks Data Science",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
