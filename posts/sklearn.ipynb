{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to pandas and sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation System\n",
    "We live in a world surrounded by recommendation systems - our shopping habbits, our reading habits, political opinions are heavily influenced by recommendation algorithms. So lets take a closer look at how to build a basic recommendation system.\n",
    "\n",
    "Simply put a recommendation system learns from your previous behavior and tries to recommend items that are similar to your previous choices. While there are a multitude of approaches for building recommendation systems, we will take a simple approach that is easy to understand and has a reasonable performance.\n",
    "\n",
    "For this exercise we will build a recommendation system that predicts which talks you'll enjoy at a conference - specifically our favorite conference Pycon!\n",
    "\n",
    "### Before you proceed\n",
    "This project is still in alpha stage. Bugs, typos, spelling, grammar, terminologies - there's every scope of finding bugs. If you have found one - [open an issue on github](https://github.com/chicagopython/CodingWorkshops/issues/new). Pull Requests with corrections, fixes and enhancements will be received with open arms! Don't forget to add yourself to the [list of contributors to this project](https://github.com/chicagopython/CodingWorkshops/blob/master/README.md). \n",
    "\n",
    "\n",
    "#### Recommendation for Pycon talks\n",
    "Take a look at 2018 [schedule](https://us.pycon.org/2018/schedule/).\n",
    "With 32 tuotorials, 12 sponsor workshops, 16 talks at the education summit, and 95 talks at the main conference - Pycon has a lot to offer. Reading through all the talk descriptions and filtering out the ones that you should go to is a tedious process. \n",
    "Lets build a recommendation system that recommends talks from Pycon 2018, based on the ones that a person went to in 2017. This way the attendee does not waste any time deciding which talk to go to and spend more time making friends on the hallway track! \n",
    "\n",
    "We will be using [`pandas`](https://pandas.pydata.org/) and [`scikit-learn`](http://scikit-learn.org/) to build the recommnedation system using the text description of talks.\n",
    "\n",
    "\n",
    "\n",
    "### Definitions\n",
    "#### Documents\n",
    "In our example the talk descriptions make up the documents\n",
    "#### Class\n",
    "We have two classes to classify our documents\n",
    "- The talks that the attendee would like to see \"in person\". Denoted by 1\n",
    "- The talks that the attendee would watch \"later online\". Denoted by 0\n",
    "\n",
    "A talk description is labeled 0 would mean the user has chosen to watch it later and a label 1 would mean the user has chose to watch it in person.\n",
    "\n",
    "### Supervised Learning\n",
    "In Supervised learning we inspect each observation in a given dataset and manually label them. These manually labeled data is used to construct a model that can predict the labels on new data. We will use a Supervised Learning technique called Support Vector Machines.\n",
    "\n",
    "In unsupervised learning we do not need any manual labeling. The recommendation system finds the pattern in the data to build a model that can be used for recommendation.\n",
    "\n",
    "### Dataset\n",
    "The dataset contains the talk description and speaker details from Pycon 2017 and 2018. All the 2017 talk data has been labeled by a user who has been to Pycon 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages installation\n",
    "The following packages are needed for this project. Execute the cell below to install them. \n",
    "\n",
    "    numpy==1.14.2\n",
    "    pandas==0.22.0\n",
    "    python-dateutil==2.7.2\n",
    "    pytz==2018.4\n",
    "    scikit-learn==0.19.1\n",
    "    scipy==1.0.1\n",
    "    six==1.11.0\n",
    "    sklearn==0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A: Load the data\n",
    "The data directory contains the snapshot of one such user's labeling - lets load that up and start with our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>presenters</th>\n",
       "      <th>date_created</th>\n",
       "      <th>date_modified</th>\n",
       "      <th>location</th>\n",
       "      <th>talk_dt</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5 ways to deploy your Python web app in 2017</td>\n",
       "      <td>You’ve built a fine Python web application and...</td>\n",
       "      <td>Andrew T. Baker</td>\n",
       "      <td>2018-04-19 00:59:20.151875</td>\n",
       "      <td>2018-04-19 00:59:20.151875</td>\n",
       "      <td>Portland Ballroom 252–253</td>\n",
       "      <td>2017-05-08 15:15:00.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A gentle introduction to deep learning with Te...</td>\n",
       "      <td>Deep learning's explosion of spectacular resul...</td>\n",
       "      <td>Michelle Fullwood</td>\n",
       "      <td>2018-04-19 00:59:20.158338</td>\n",
       "      <td>2018-04-19 00:59:20.158338</td>\n",
       "      <td>Oregon Ballroom 203–204</td>\n",
       "      <td>2017-05-08 16:15:00.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aiosmtpd - A better asyncio based SMTP server</td>\n",
       "      <td>smtpd.py has been in the standard library for ...</td>\n",
       "      <td>Barry Warsaw</td>\n",
       "      <td>2018-04-19 00:59:20.161866</td>\n",
       "      <td>2018-04-19 00:59:20.161866</td>\n",
       "      <td>Oregon Ballroom 203–204</td>\n",
       "      <td>2017-05-08 14:30:00.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Algorithmic Music Generation</td>\n",
       "      <td>Music is mainly an artistic act of inspired cr...</td>\n",
       "      <td>Padmaja V Bhagwat</td>\n",
       "      <td>2018-04-19 00:59:20.165526</td>\n",
       "      <td>2018-04-19 00:59:20.165526</td>\n",
       "      <td>Portland Ballroom 251 &amp; 258</td>\n",
       "      <td>2017-05-08 17:10:00.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>An Introduction to Reinforcement Learning</td>\n",
       "      <td>Reinforcement learning (RL) is a subfield of m...</td>\n",
       "      <td>Jessica Forde</td>\n",
       "      <td>2018-04-19 00:59:20.169075</td>\n",
       "      <td>2018-04-19 00:59:20.169075</td>\n",
       "      <td>Portland Ballroom 252–253</td>\n",
       "      <td>2017-05-08 13:40:00.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1       5 ways to deploy your Python web app in 2017   \n",
       "1   2  A gentle introduction to deep learning with Te...   \n",
       "2   3      aiosmtpd - A better asyncio based SMTP server   \n",
       "3   4                       Algorithmic Music Generation   \n",
       "4   5          An Introduction to Reinforcement Learning   \n",
       "\n",
       "                                         description         presenters  \\\n",
       "0  You’ve built a fine Python web application and...    Andrew T. Baker   \n",
       "1  Deep learning's explosion of spectacular resul...  Michelle Fullwood   \n",
       "2  smtpd.py has been in the standard library for ...       Barry Warsaw   \n",
       "3  Music is mainly an artistic act of inspired cr...  Padmaja V Bhagwat   \n",
       "4  Reinforcement learning (RL) is a subfield of m...      Jessica Forde   \n",
       "\n",
       "                 date_created               date_modified  \\\n",
       "0  2018-04-19 00:59:20.151875  2018-04-19 00:59:20.151875   \n",
       "1  2018-04-19 00:59:20.158338  2018-04-19 00:59:20.158338   \n",
       "2  2018-04-19 00:59:20.161866  2018-04-19 00:59:20.161866   \n",
       "3  2018-04-19 00:59:20.165526  2018-04-19 00:59:20.165526   \n",
       "4  2018-04-19 00:59:20.169075  2018-04-19 00:59:20.169075   \n",
       "\n",
       "                      location                     talk_dt  year  label  \n",
       "0    Portland Ballroom 252–253  2017-05-08 15:15:00.000000  2017    0.0  \n",
       "1      Oregon Ballroom 203–204  2017-05-08 16:15:00.000000  2017    0.0  \n",
       "2      Oregon Ballroom 203–204  2017-05-08 14:30:00.000000  2017    1.0  \n",
       "3  Portland Ballroom 251 & 258  2017-05-08 17:10:00.000000  2017    0.0  \n",
       "4    Portland Ballroom 252–253  2017-05-08 13:40:00.000000  2017    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('talks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief description of the interesting fields.\n",
    "\n",
    "variable | description  \n",
    "------|------|\n",
    "`title`|Title of the talk\n",
    "`description`|Description of the talk\n",
    "`year`|Is it a `2017` talk or `2018`  \n",
    "`label`|`1` indicates the user preferred seeing the talk in person,<br> `0` indicates they would schedule it for later.\n",
    "\n",
    "Note all 2018 talks are set to 1. However they are only placeholders, and are not used in training the model. We will  use 2017 data for training, and predict the labels on the 2018 talks.\n",
    "\n",
    "Lets start by selecting the 2017 talk descriptions that were labeled by the user for watching in person.\n",
    "\n",
    "```python\n",
    "df[(df.year==2017) & (df.label==1)]['description']\n",
    "```\n",
    "\n",
    "Print the description of the talks that the user preferred watching in person. How many such talks are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Select 2017 talk description and labels from the Pandas dataframe. How many of them are present? Do the same for 2018 talks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2017 talks will be used for training and the 2018 talks will we used for predicting. Set the values of `year_labeled` and `year_predict` to appropriate values and print out the values of `description_labeled` and `description_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_labeled=\n",
    "year_predict=\n",
    "description_labeled = df[df.year==year_labeled]['description']\n",
    "description_predict = df[df.year==year_predict]['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Introduction to Text Analysis\n",
    "![text-analysis](text-analysis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a quick overview of text analysis. Our end goal is to train a machine learning algorithm by making it go through enough documents from each class to recognize the distingusihing characteristics in documents from a particular class. \n",
    "\n",
    "1. *Labeling* - This is the step where the user (i.e. a human) reviews a set of documents and manually classifies them. For our problem, here a Pycon attendee is labeling a talk description from 2017 as \"watch later\"(0) or \"watch now\" (1).\n",
    "1. *Training/Testing split* - In order to test our algorithm, we split parts of our labeled data into training (used to train the algorithm) and testing set (used to test the algorithm).\n",
    "1. *Vectorization & feature extraction* - Since machine learning algorithms deal with numbers rather than words, we vectorize our documents - i.e. we split the documents into individual unique words and count the frequency of their occurance across documents. There are different data normalization is possible at this stage like stop words removal, [lemmatization](https://spacy.io/api/lemmatizer) - but we will skip them for now. Each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "1. *Model training* - This is where we build the model.\n",
    "1. *Model testing* - Here we test out the model to see how it is performing against label data as we subject it to the previously set aside test set.\n",
    "1. *Tweak and train* - If our measures are not satisfactory, we will change the parameters that define different aspects of the machine learning algorithm and we will train the model again.\n",
    "1. Once satisfied with the results from the previous step, we are now ready to deploy the model and have new unlabled documents be classified by it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Vectorize and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we build the feature set by tokenization, counting and normalization of the bi-grams from the text descriptions of the talk.\n",
    "\n",
    "**tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators\n",
    "\n",
    "**counting** the occurrences of tokens in each document\n",
    "\n",
    "**normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents\n",
    "\n",
    "You can find more information on text feature extraction [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) and TfidfVectorizer [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit\n",
    "Note that we are choosing default value on all parameters for `TfidfVectorizer`. While this is a starting point, for better results we would want to come back and tune them to reduce noise. You can try that after you have taken a first pass through all the exercises. You might consider using [spacy](https://spacy.io/api/lemmatizer) to fine tune the input to `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 Fit_transform\n",
    "We will use the [fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform) method to learn the vocabulary dictionary and return term-document matrix. What should be the input to `fit_transform`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text_labeled = vectorizer.fit_transform( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 Inspect the vocabulary\n",
    "Take a look at the vocabulary dictionary that is accessible by calling `vocabulary_` on the `vectorizer`. The stopwords can be accessed using `stop_words_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `get_feature_names` function on the Tfidf `vectorizer` to get the features (terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = np.asarray(vectorized_text_labeled.sum(axis=0)).ravel()\n",
    "terms = ( ... )\n",
    "counts_df = pd.DataFrame({'terms': terms, 'occurrences': occurrences}).sort_values('occurrences', ascending=False)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 Transform documents for prediction into document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data on which we will do our predictions, we will use the [transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform) method to get the document-term matrix.\n",
    "We will use this later, once we have our model ready. What should be the input to the `transform` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text_predict = vectorizer.transform( ... )\n",
    "vectorized_text_predict.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Split into training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into training set and testing set. This allows us to do cross validation and avoid overfitting. Use the `train_test_split` method from `sklearn.model_selection` to split the `vectorized_text_labeled` into training and testing set with the test size as one third of the size (0.3) of the labeled.\n",
    "\n",
    "[Here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is the documentation for the function. The example usage should be helpful for understanding what `X_train, X_test, y_train, y_test` tuple represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = df[df.year == 2017]['label']\n",
    "test_size= ...\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 Inspect the shape of each output of train_test_split\n",
    "For each of the output above, get the shape of the matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Train the model\n",
    "Finally we get to the stage for training the model. We are going to use a linear [support vector classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) and check its accuracy by using the `classification_report` function. Note that we have not done any parameter tuning done yet, so your model might not give you the best results. Like `TfIdfVectorizer` you can come back and tune these parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(verbose=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model by using the the `classification_report` method from the [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). What are the values of precision, recall and f1-scores? They are defined [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict( ... )\n",
    "report = sklearn.metrics.classification_report( ... , ... )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Make Predictions\n",
    "Use the model to predict which 2018 talks the user should go to. Plugin `vectorized_text_predict` from exercise 2.3 to get the `predicted_talks_vector` into the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_talks_vector = classifier.predict( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `predicted_talk_indexes` get  the talk id, description, presenters, title and location and talk date.\n",
    "How many talks should the user go to according to your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = df[df.year==2018]\n",
    "predicted_talk_indexes = predicted_talks_vector.nonzero()[0] + len(df[df.year==2017])\n",
    "\n",
    "df_2018_talks = df_2018.loc[predicted_talk_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "You might not be very happy with the results. You might want to reduce the manual steps for tuning the parameters. So where do you go from here?\n",
    "There are three specific next steps that can make this better.\n",
    "* [Spacy](https://spacy.io/) - This is an industrial strength natural language processeing libray that has a friendly api. This would be useful in your feature extraction steps.\n",
    "* Try using a different algorithm. [There is a lot](http://scikit-learn.org/stable/supervised_learning.html) to choose from.\n",
    "* [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) together make a great combination for automating the process of searching for the best models and parameters that accurately represent the patterns in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nikola": {
   "category": "",
   "date": "2019-03-17 11:43:40 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "introduction-to-text-analysis-with-sklearn",
   "tags": "",
   "title": "Introduction to Text Analysis with sklearn",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
