<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python Project Night Challenges (Posts about data-science)</title><link>https://chicagopython.github.io/</link><description></description><atom:link href="https://chicagopython.github.io/categories/cat_data-science.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:chicago-sig-organizers@python.org"&gt;Chicago Python User Group&lt;/a&gt; 
&lt;a rel="license" href="https://www.gnu.org/licenses/gpl-3.0.en.html"&gt;
&lt;img alt="Gnu Public License version 3.0"
style="border-width:0;"
src="https://www.gnu.org/graphics/gplv3-with-text-84x42.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Thu, 16 Jan 2020 02:47:35 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Fuzzy String Matching</title><link>https://chicagopython.github.io/posts/fuzzy-string-matching/</link><dc:creator>Chicago Python User Group</dc:creator><description>&lt;div&gt;&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Data collection has and is rapidly expanding. However, data often isn’t submitted and/or collected without the required cleanliness or detail. At ChiPy we face the issue of trying to match Project Night attendees’ Meetup names with their legal names, which are needed for venue security. As a human it’s often easy to tell when names with slight variations match (Mike vs Michael; missing initials, etc), but trying to match hundreds of names one at a time is time consuming. Your job is to match the meetup and given names as accurately as possible using the fuzzy matching technique(s) of your choosing.&lt;/p&gt;
&lt;p&gt;Background reading:&lt;br&gt;
- &lt;a href="http://www.basistech.com/whitepapers/the-name-matching-you-need-EN.pdf"&gt;The Name Matching You Need: A Comparison of Name Matching Technologies&lt;/a&gt;&lt;br&gt;
- &lt;a href="https://medium.com/bcggamma/an-ensemble-approach-to-large-scale-fuzzy-name-matching-b3e3fa124e3c"&gt;An Ensemble Approach to Large-Scale Fuzzy Name Matching&lt;/a&gt;&lt;br&gt;
- &lt;a href="https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536"&gt;Fuzzy Matching at Scale&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Some Python libraries you might want to use:&lt;br&gt;
- &lt;a href="https://pypi.org/project/fuzzywuzzy/"&gt;fuzzywuzzy&lt;/a&gt;&lt;br&gt;
- &lt;a href="https://pypi.org/project/textdistance/"&gt;textdistance&lt;/a&gt;&lt;br&gt;
- &lt;a href="https://github.com/Bergvca/string_grouper"&gt;string_grouper&lt;/a&gt;  &lt;/p&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;p&gt;There is no existing repo for this project, and no requirements to install. All you need to start is the data, which can be downloaded &lt;a href="https://drive.google.com/file/d/1WtW89K43Rwxq5ZM8Dyryv5EQgkkauOCF/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to work how you see fit. That said, we strongly recommend setting up a virtual environment.&lt;/p&gt;
&lt;p&gt;If you are using Linux or OS X, run the following to create a new virtualenv:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;
&lt;span class="k"&gt;source&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;On Windows, instead run the following:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;
&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Scripts&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;/pre&gt;


&lt;h3&gt;So what should we do?&lt;/h3&gt;
&lt;p&gt;The dataset has three columns: 
- meetup_id: The unique Meetup identifier for each user.&lt;br&gt;
- meetup_name: The publicly available display name of the Meetup user.&lt;br&gt;
- given_names: The "actual" name of the attendee, as given as a form response via Meetup.  &lt;/p&gt;
&lt;p&gt;Each row in the dataset has the True matching name. In some cases, the meetup and given names match exactly, in some cases they don't. You won't need the meetup_id while actually attempting to match meetup and given names, but you can use it to validate your approach.&lt;/p&gt;
&lt;p&gt;Some things you might want to consider along the way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Are all the names usable? Could a human uniquely identify matches?&lt;/li&gt;
&lt;li&gt;What patterns can be identified in the data we're working with?&lt;/li&gt;
&lt;li&gt;What is our true goal with matching? In other words, when evaluating our process' success, how do we balance ensuring someone has preregistered with not turning too many people away at the door? To that end, what's the right evaluation metric to choose?&lt;/li&gt;
&lt;li&gt;How might our approach differ if instead of a couple hundred names we have 10,000, a million, or even a billion names to match?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Happy Developing!&lt;/p&gt;&lt;/div&gt;</description><guid>https://chicagopython.github.io/posts/fuzzy-string-matching/</guid><pubDate>Thu, 16 Jan 2020 11:00:00 GMT</pubDate></item><item><title>Predict Home Credit Defaults</title><link>https://chicagopython.github.io/posts/home-credit-default-risk/</link><dc:creator>Chicago Python User Group</dc:creator><description>&lt;div&gt;&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.&lt;/p&gt;
&lt;p&gt;Tonight's project examines a dataset from a real bank that focuses on lending to people with little or no credit history. Their goal is to ensure that clients capable of repayment are not rejected. You will explore the dataset and make predictions whether someone will default or not, based on their application for a loan.&lt;/p&gt;
&lt;h3&gt;Your Task&lt;/h3&gt;
&lt;p&gt;Your goal is to train a binary classification model on the data in &lt;code&gt;default_risk_train_data.csv&lt;/code&gt; that optimized area under the ROC curve between the predicted probability and the observed target. For each &lt;code&gt;SK_ID_CURR&lt;/code&gt; in &lt;code&gt;default_risk_train_data.csv&lt;/code&gt;, you must predict a probability for the TARGET variable. Your deliverable to the bank will be a CSV with predictions for each SK_ID_CURR in the test set.&lt;/p&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For this challenge you will need Python 3.7, pipenv, and git installed. If you're not familiar with pipenv, it's a packaing tool for Python that effectively replaced the pip+virtualenv+requirements.txt workflow. If you already have pip installed, the easiest way to install pipenv is with &lt;code&gt;pip install --user pipenv&lt;/code&gt;; however, a better way for Mac/Linux Homebrew users is to instead run &lt;code&gt;brew install pipenv&lt;/code&gt;. More options can be found &lt;a href="https://pipenv-fork.readthedocs.io/en/latest/install.html#installing-pipenv"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The project is in the ChiPy project night repo. If you do not have the repository already, run &lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/chicagopython/CodingWorkshops.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the folder for this challenge:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd CodingWorkshops/problems/data_science/home_credit_default_risk&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;pipenv install&lt;/code&gt;, which will install all of the libraries we have recommended for this exercise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;After you've installed all of the libraries, run &lt;code&gt;pipenv shell&lt;/code&gt;, which will turn on a virtual environment running Python 3.7.&lt;/li&gt;
&lt;li&gt;From within the shell, run &lt;code&gt;jupyter lab default_risk.ipynb&lt;/code&gt; to launch the pre-started notebook.&lt;/li&gt;
&lt;li&gt;To exit the pipenv shell when you are done, simply type &lt;code&gt;exit&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What's in this repository?&lt;/h3&gt;
&lt;p&gt;There are three data files, one metadata file, and a jupyter notebook.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;default_risk_train_data.csv -- The data you will use to train your models. Includes all potential features and the target.&lt;/li&gt;
&lt;li&gt;default_risk_test_data.csv -- The data you will use to test your models. Includes all potential features, but NOT the target (which theoretically reflect unknown future default status).&lt;/li&gt;
&lt;li&gt;perfect_deliverable.csv -- The CSV with perfect predictions for each SK_ID_CURR in the test set. You should only use this at the very end to test the model and NEVER factor it into training your model. To prevent overfitting, you should test models sparingly. This is the same format the final deliverable should be submitted to the bank in.&lt;/li&gt;
&lt;li&gt;default_risk_column_descriptions.csv -- Descriptive metadata for the columns found in the train and test datasets.&lt;/li&gt;
&lt;li&gt;default_risk.ipynb -- The jupyer notebook where all coding should be completed, unless you opt to work in a different environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This project is based on a Kaggle competition, with a subset of the data provided for the sake of download size. Note that this data has not been cleaned for you, and you should expect to deal with real world data issues, such as missing values, bad values, class imbalances, etc.&lt;/p&gt;
&lt;h3&gt;So what should we do?&lt;/h3&gt;
&lt;p&gt;To successfully complete this challenge, you'll need to:
    1. become an expert on the data,
    2. clean the data,
    3. engineer the features for your model(s),
    4. test/validate your models,
    5. generate the deliverable the bank expects.&lt;/p&gt;
&lt;p&gt;Here are some tips/questions to consider along the way:
- Identify which columns are numerical and which are categorical
- Which columns are missing values, and what should be done about the missing values?
- Which features are relevant and why?
- Which features might you want to remove?
- What new features might you create?
- How will you deal with categorical data (e.g. Label Encoding, One-Hot encoding, etc).
- Is there any class imbalance?
- What models will you try? sklearn has been installed in your environment; and &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"&gt;linear regression&lt;/a&gt;, &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"&gt;logistic regression&lt;/a&gt;, and &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"&gt;random forest&lt;/a&gt; models have been imported in the given notebook. Feel free, however, to use the library/models of your choice.&lt;/p&gt;&lt;/div&gt;</description><guid>https://chicagopython.github.io/posts/home-credit-default-risk/</guid><pubDate>Thu, 17 Oct 2019 11:00:00 GMT</pubDate></item><item><title>GitHub Jobs API</title><link>https://chicagopython.github.io/posts/github-jobs-api/</link><dc:creator>Chicago Python User Group</dc:creator><description>&lt;div&gt;&lt;h3&gt;Project Night Purpose&lt;/h3&gt;
&lt;p&gt;All of us need to look for a job at some point; and most every job board has its own API for users to post and pull data (though usually they charge money for access). The &lt;a href="https://jobs.github.com"&gt;GitHub Jobs page&lt;/a&gt; is a great simple API for a first look at this kind of data acqusition and analysis.&lt;/p&gt;
&lt;p&gt;Tonight's project uses the very popular Python HTTP library &lt;a href="https://2.python-requests.org/en/master/"&gt;requests&lt;/a&gt; along with &lt;a href="https://docs.python.org/3/library/json.html"&gt;json&lt;/a&gt; from the standard library. You will explore the data in the jobs API with the intent of learning something about the current job market for devs. There are a ton of caveats here: for example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who chooses to post to GitHub Jobs? Are they representative of the overall population?&lt;/li&gt;
&lt;li&gt;How old are the postings?&lt;/li&gt;
&lt;li&gt;Is it safe to extrapolate statistics to the country? to Illinois? to Chicago?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the topic is a little serious we want to make sure you don't get discouraged by the data you pull (ask around--how many people in your group got their job from a GitHub posting?). The GitHub site shouldn't be taken as a good primary source for job availability or category...but that's part of data acquistition and analysis: assessing the strenghts and shortcomings of your data source. Possible discusison points for the group are things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Where might you get additional information?&lt;/li&gt;
&lt;li&gt;How often should you pull the data? What would capture over time give you?&lt;/li&gt;
&lt;li&gt;Does the dataset tell you anything about which companies use the GitHub for hiring at all? In which cities? Should we move to a different part of the country?&lt;/li&gt;
&lt;li&gt;Choose your own adventure and share with the group what you have learned!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most importantly - have fun! Accessing an API is a great core skill for data analysis and all experience is good experience. We hope that you are creative in your explanation and that each group discovers different things!&lt;/p&gt;
&lt;h3&gt;Setting up your environment&lt;/h3&gt;
&lt;p&gt;There is no pre-written code for this project, but we assume you have Python 3.+ installed on your machine.  If this is your fisrt project night, we recommend creating a folder for the project night repo: &lt;code&gt;mkdir chipy_projects &amp;amp;&amp;amp; cd chipy_projects&lt;/code&gt;. If you already have the project night repository on your machine, go to that directory and pull from master.&lt;/p&gt;
&lt;p&gt;If you are using Linux or OS X, run the following to create a new virtualenv:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;github_jobs_api&lt;/span&gt;
&lt;span class="k"&gt;source&lt;/span&gt; &lt;span class="n"&gt;github_jobs_api&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;On Windows, run the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;github_jobs_api&lt;/span&gt; 
&lt;span class="n"&gt;github_jobs_api&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Scripts&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/pre&gt;


&lt;h3&gt;Getting the project&lt;/h3&gt;
&lt;p&gt;The project is in the ChiPy project night repo. If you do not have the repository already, run &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;chicagopython&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;CodingWorkshops&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now we will:&lt;/p&gt;
&lt;p&gt;Go to the project:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cd&lt;/span&gt; &lt;span class="n"&gt;CodingWorkshops&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;problems&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;data_science&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;github_jobs_api&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Install the packages we need into our environment:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;view the &lt;em&gt;README.md&lt;/em&gt; file for additional information:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;README&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;
&lt;/pre&gt;


&lt;h3&gt;Have fun!&lt;/h3&gt;&lt;/div&gt;</description><category>api</category><category>EDA</category><guid>https://chicagopython.github.io/posts/github-jobs-api/</guid><pubDate>Thu, 15 Aug 2019 23:00:00 GMT</pubDate></item><item><title>Battery Life</title><link>https://chicagopython.github.io/posts/battery-life/</link><dc:creator>Chicago Python User Group</dc:creator><description>&lt;div&gt;&lt;h2&gt;Predicting Battery Life - Challenge #1: Gathering Data&lt;/h2&gt;
&lt;p&gt;Portable electronics such as mobile phones and laptops have become a near necessity in our daily lives; and those devices share one essential resource in common: battery life. Have you ever sat on the floor to be by a power outlet for your laptop or cell phone? How about delayed leaving for an event because you had to make sure your phone was charged? In a perfect world, we wouldn't have to worry about battery life, but in the absence of the miracle battery, users must rely on indicators of remaining battery life.&lt;/p&gt;
&lt;p&gt;While there's still ongoing research into the capacity of batteries over time, the question of what percentage of charge remains has largely been solved in our everyday electronics. Most operating systems offer a way to display the percentage of battery life remaining. However, features that predict time remaining on the battery have been notoriously inaccurate, to the point where such features have been removed or hidden by default. Wouldn't it be nice if we could accurately predict when our phone was going to "die?"&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your goal is going to be work toward that solution by gathering data to build a machine learning model predicting remaining battery life. You are tasked with determining what data we might want to collect for such a model, determining a strategy for ongoing collection of that data, actually collecting it, and organizing it into a form that will be usable to the machine-learning models of choice.&lt;/strong&gt; There are no right or wrong answer here, just things that are feasible and ultimately help drive better predictions.&lt;/p&gt;
&lt;p&gt;Before digging in, some background reading on how batteries work and what kinds of data/models can be useful is likely in order. Feel free to find your own resources, but here are a few:
- Overview of research and features: https://arxiv.org/pdf/1801.04069.pdf
- Battery Terminology: http://web.mit.edu/evt/summary_battery_specifications.pdf
- Battery Discharge Formulas: https://planetcalc.com/2283/&lt;/p&gt;
&lt;p&gt;Once you're ready to collect data, you'll likely want to collect running process and/or system utilization data as at least part of the data you collect. Gathering such data can vary drastically by hardware and operating system. To get you started, here are a few options to make extracting the data easier: 
- The &lt;a href="https://psutil.readthedocs.io/en/latest/"&gt;psutil&lt;/a&gt; library in python has cross-OS support, but only collects some such data.
- On Windows, there's the &lt;a href="http://timgolden.me.uk/python/wmi/tutorial.html"&gt;wmi&lt;/a&gt; library.
- On most distributions of Linux and MacOS, the standard librarys' os, sys, and subprocess modules can actually get you rolling pretty quickly, once you track down where system logs are stored!&lt;/p&gt;
&lt;p&gt;The rest is up to you, but some questions you might want to consider:
- When tracking battery/system data, how are you accounting for a device sometimes being plugged in?
- How will you account for different battery types, device types, and operating systems?
- Besides the obvious battery and system-related data, what features might help predict battery life?
- How can you collect enough data from enough sources to successfully  train a model?&lt;/p&gt;&lt;/div&gt;</description><guid>https://chicagopython.github.io/posts/battery-life/</guid><pubDate>Thu, 18 Jul 2019 11:00:00 GMT</pubDate></item><item><title>ChiPy Chipmunk Project Night </title><link>https://chicagopython.github.io/posts/chipy-chipmunks/</link><dc:creator>Chicago Python User Group</dc:creator><description>&lt;div&gt;&lt;h3&gt;Project Night Purpose&lt;/h3&gt;
&lt;p&gt;Many people assume data scientists spend all day visualizing data and making impressive predictive models. While this isn’t untrue, the luckiest and most productive data scientists spend a lot of their time communicating. They communicate their model results - as well as their assumptions and limitations when making their models and doing analysis - in a way that is digestible to their stakeholders and colleagues. &lt;/p&gt;
&lt;p&gt;Tonight’s project is aimed towards that aspect of communication. You will be asked to make assumptions as a team - particularly as they pertain to this problem and what the stakeholders need. There are no exactly correct assumptions or answers for this project night. There may be assumptions and answers that clearly don’t have evidence to support them, but do not feel bogged down by getting the “right” answer.&lt;/p&gt;
&lt;p&gt;Most importantly - have fun. While this project night covers serious concepts, it is ridiculously silly and meant to be taken with a bit of lighthearted exploration and plenty of opportunities to make mistakes.&lt;/p&gt;
&lt;h3&gt;Setting up your environment&lt;/h3&gt;
&lt;p&gt;This project is contained in a jupyter notebook and is assuming you have Python 3.+ installed on your machine. If this is your fisrt project night, we recommend creating a folder for the project night repo: &lt;code&gt;mkdir chipy_projects &amp;amp;&amp;amp; cd chipy_projects&lt;/code&gt;. If you already have the project night repository on your machine, go to that directory and pull from master.&lt;/p&gt;
&lt;p&gt;If you are using Linux or OS X, run the following to create a new virtualenv:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;chipmunk&lt;/span&gt;
&lt;span class="k"&gt;source&lt;/span&gt; &lt;span class="n"&gt;chipmunk&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;On Windows, run the following&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;chipmunk&lt;/span&gt; 
&lt;span class="n"&gt;chipmunk&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Scripts&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/pre&gt;


&lt;h3&gt;Getting the project&lt;/h3&gt;
&lt;p&gt;The project is in the ChiPy project night repo. If you do not have the repository already, run &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;chicagopython&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;CodingWorkshops&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now we will:&lt;/p&gt;
&lt;p&gt;Go to the project:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cd&lt;/span&gt; &lt;span class="n"&gt;CodingWorkshops&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;problems&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;data_science&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;chipmunks&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Install the packages we need into our environment:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Run our jupyter notebook server for the project:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;jupyter&lt;/span&gt; &lt;span class="n"&gt;notebook&lt;/span&gt;
&lt;/pre&gt;


&lt;h3&gt;Have fun!&lt;/h3&gt;&lt;/div&gt;</description><category>analytics</category><category>EDA</category><category>pandas</category><category>statistics</category><guid>https://chicagopython.github.io/posts/chipy-chipmunks/</guid><pubDate>Thu, 18 Apr 2019 23:00:00 GMT</pubDate></item></channel></rss>